# AI Usage Disclosure — Phase 2

## Tools Used

| Tool | What I Used It For | What I Changed Manually |
|------|-------------------|------------------------|
| Claude Code (Claude Opus 4.6) | Scaffolded the RAG pipeline architecture: ingestion, retrieval, generation, guardrails, and evaluation modules. Generated initial drafts of `pdf_parser.py`, `chunker.py`, `retriever.py`, `reranker.py`, `generator.py`, `guardrails.py`, `pipeline.py`, and `evaluator.py`. | Reviewed all generated code for correctness. Revised the system prompt through multiple iterations based on evaluation results (e.g., added quote-or-omit rule, failure-mode checklist). Tuned retrieval hyperparameters (chunk size, overlap, similarity threshold, RRF weights). Removed the faithfulness filter guardrail after testing showed it was unreliable. Adjusted temperature from 0.1 to 0.0. |
| Claude Code (Claude Opus 4.6) | Drafted the evaluation report structure, per-query results tables, and failure-case analysis sections. | Wrote all interpretive analysis (failure-mode explanations, enhancement comparison, lessons learned) myself. Verified that reported metrics matched the raw evaluation JSON. |
| Claude Code (Claude Opus 4.6) | Generated `build_manifest.py` to auto-extract PDF metadata and bootstrap `data_manifest.csv`. | Manually verified and corrected metadata fields (authors, year, venue, DOI) for all 18 sources. Filled in fields marked "MANUAL" by the script. Wrote all relevance notes. |
| Cursor (Claude Opus 4.6 agent) | Identified and fixed logic bugs: double-retrieval in `evaluator.py`, incorrect `no_evidence_accuracy` scoring (string-match vs. explicit boolean), and Makefile path-quoting issues. | Reviewed each diagnosis and confirmed the root cause before accepting fixes. |
| Cursor (Claude Opus 4.6 agent) | Implemented new pipeline enhancements: LLM reranker (`reranker.py`), chunk expansion in `retriever.py`, topic-presence check and entity-check guardrails in `guardrails.py`, and pipeline orchestration updates in `pipeline.py`. | Made final decisions on enhancement selection (LLM reranker over cross-encoder) and config values (`rerank_candidates=10`, `final_k=6`, `chunk_expand_window=1`). |
| Cursor (Claude Opus 4.6 agent) | Iterated on the generator system prompt: added explicit failure-mode warnings (knowledge leakage, specificity inflation, topic hallucination, over-reliance on one chunk), scan-all-chunks instruction, precise confidence calibration criteria, mandatory verbatim quotes, and avoid-generic-filler rule. Improvements were informed by Phase 1 prompt kit findings. | Provided the Phase 1 analysis memo and prompt kit as context and directed which improvements to apply. |
| Cursor (Claude Opus 4.6 agent) | Updated the evaluation report (`evaluation_report.md`) with new results, failure-case analysis, enhancement comparison narrative, and lessons-learned section. Removed duplicate source from `data_manifest.csv` and updated query target papers. | Verified all reported metrics against raw JSON. Wrote interpretive analysis and design rationale. |
| Cursor (Claude Opus 4.6 agent) | Switched the LLM model from GPT-4.1 to GPT-5-mini (`gpt-5-mini-2025-08-07`) in `config.py` and updated the README architecture diagram and enhancements table. | Decided on the model switch based on assignment requirements and cost considerations. |
| ChatGPT (GPT-4.1) | Used as a secondary reference when debugging chunking edge cases and understanding PyMuPDF font-size extraction. | All code changes were made by me based on the information gathered. |

## Summary

AI tools were used for code scaffolding, bug diagnosis and fixing, enhancement implementation, prompt engineering, and report drafting. **Cursor** (with Claude Opus 4.6 agent) was used extensively for iterative development — identifying issues in the evaluation pipeline, implementing new guardrails and reranking, and refining the generator prompt based on Phase 1 lessons. All design decisions (prompt strategy, guardrail selection, enhancement choices, hyperparameter tuning, evaluation query design) were made by me. All generated code was reviewed, tested, and revised manually. The corpus of 18 papers was selected and downloaded by me based on my research domain.
